{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILo7yIvLt2Q"
      },
      "source": [
        "Environment Dependent on Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoVpWFQAMsQk"
      },
      "outputs": [],
      "source": [
        "!pip install ollama\n",
        "!pip install llama-index\n",
        "!pip install llama-index-llms-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWWPWAOWML0G"
      },
      "source": [
        "Refining the Concept of an Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AEhDXITMYkW"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, ServiceContext\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "Settings.llm = Ollama(model=\"llama3:8b\", temperature=0, request_timeout=120.0)\n",
        "\n",
        "prompt_template = '''\n",
        "# Self reflection with ToT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. When answering questions, please generate a thought tree based on the content of the input question. This thought tree should contain reasoning paths, and the final answer to the question should be deduced based on multiple paths generated from the thought tree.\n",
        "2. Use as many **reasoning steps** as possible.   At least 3 steps.\n",
        "3. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "4. Include exploration of alternative answers.   Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "5. When you say you are rechecking, actually recheck and use another method.   Don't just say you are rechecking.\n",
        "6. Use at least 3 methods to arrive at the answer.\n",
        "7. Use best practices.\n",
        "8. Output the format of the answer: \"Answer\" Thinking tree: (This includes every step of the thinking tree and relation)\n",
        "\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step to verify its correctness. After completing each step, initiate a 'Review LLM' to examine the current step from different perspectives.\n",
        "1. For the answers and thought trees generated from the first step, please conduct a detailed review of each answer and thought tree, and apply a self-reflection mechanism to each step of the thought tree, using as many reasoning steps as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI, as well as what you can and cannot do.\n",
        "3. Include the exploration of different answers. Consider that you may be wrong, and identify where mistakes might occur if your reasoning is incorrect.'''\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    tools=[],\n",
        "    verbose=True,\n",
        "    system_prompt=prompt_template\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-SqtJFKPE3N"
      },
      "source": [
        "MINTQA Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8OhgwuPHX3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "\n",
        "PROMPT_1 = \"\"\"\n",
        "# Self reflection with ToT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. When answering questions, please generate a thought tree based on the content of the input question. This thought tree should contain reasoning paths, and the final answer to the question should be deduced based on multiple paths generated from the thought tree.\n",
        "2. Use as many **reasoning steps** as possible.   At least 3 steps.\n",
        "3. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "4. Include exploration of alternative answers.   Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "5. When you say you are rechecking, actually recheck and use another method.   Don't just say you are rechecking.\n",
        "6. Use at least 3 methods to arrive at the answer.\n",
        "7. Use best practices.\n",
        "8. Output the format of the answer: \"Answer\" Thinking tree: (This includes every step of the thinking tree and relation)\n",
        "\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step to verify its correctness. After completing each step, initiate a 'Review LLM' to examine the current step from different perspectives.\n",
        "1. For the answers and thought trees generated from the first step, please conduct a detailed review of each answer and thought tree, and apply a self-reflection mechanism to each step of the thought tree, using as many reasoning steps as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI, as well as what you can and cannot do.\n",
        "3. Include the exploration of different answers. Consider that you may be wrong, and identify where mistakes might occur if your reasoning is incorrect.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_2 = \"\"\"Hi there\"\"\"\n",
        "\n",
        "def create_answer_extraction_agent(llm):\n",
        "\n",
        "    def extract_final_number(text: str) -> str:\n",
        "        if \"####\" in text:\n",
        "            match = re.search(r\"####\\s*(-?\\d*\\.?\\d+)\", text)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        numbers = re.findall(r\"(-?\\d*\\.?\\d+)\", text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    extract_tool = FunctionTool.from_defaults(\n",
        "        fn=extract_final_number,\n",
        "        name=\"extract_final_number\",\n",
        "        description=\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\"\n",
        "    )\n",
        "\n",
        "    agent = ReActAgent.from_tools(\n",
        "        tools=[extract_tool],\n",
        "        llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return agent\n",
        "\n",
        "# 请下载 MintQA 数据集，然后放到 dataset 目录下\n",
        "# 数据下载链接：https://github.com/probe2/multi-hop\n",
        "def load_mintqa_dataset(path=\"./dataset/MINTQA-TI.json\", num_samples=50):\n",
        "    questions = []\n",
        "    answers = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            questions.append(data['question'])\n",
        "            answer = data['answer'].split('####')[-1].strip()\n",
        "            answers.append(answer)\n",
        "            if len(questions) >= num_samples:\n",
        "                break\n",
        "    return questions[:5], answers[:5]\n",
        "\n",
        "def evaluate_prompt(system_prompt, questions, answers):\n",
        "    llm = Ollama(\n",
        "        model=\"llama3:8b\",  # llama3:8b, qwen2:7b, phi3:8b, gemma2:9b\n",
        "        temperature=0,\n",
        "        base_url=API_BASE,  # ollama default localhost: \"http://localhost:11434\" \n",
        "        system_prompt=system_prompt\n",
        "    )\n",
        "\n",
        "    agent = create_answer_extraction_agent(llm)\n",
        "\n",
        "    correct = 0\n",
        "    responses = []\n",
        "    extracted_answers = []\n",
        "\n",
        "    for q, a in tqdm(zip(questions, answers), total=len(questions)):\n",
        "        try:\n",
        "            response = llm.complete(q)\n",
        "            responses.append(response.text)\n",
        "\n",
        "            agent_response = agent.chat(\n",
        "                f\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\\n{response.text}\"\n",
        "            )\n",
        "            extracted_answer = agent_response.response.strip()\n",
        "            extracted_answers.append(extracted_answer)\n",
        "\n",
        "            if str(extracted_answer).strip() == str(a).strip():\n",
        "                correct += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {e}\")\n",
        "            responses.append(\"Error\")\n",
        "            extracted_answers.append(\"Error\")\n",
        "\n",
        "    accuracy = correct / len(questions)\n",
        "    return accuracy, responses, extracted_answers\n",
        "\n",
        "def main():\n",
        "    iteration = 5 # 3,5,7\n",
        "    questions, answers = load_mintqa_dataset(num_samples=50)\n",
        "\n",
        "    print(\"Testing Prompt 1...\")\n",
        "    accuracy1, responses1, extracted1 = evaluate_prompt(PROMPT_1, questions, answers)\n",
        "\n",
        "    print(\"Testing Prompt 2...\")\n",
        "    accuracy2 = None\n",
        "    response2 = None\n",
        "    extracted2 = None\n",
        "    for i in range(iteration):\n",
        "        if(response2 is not None):\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2 + \"\\n\" + \"Generated response:\" + response2, questions, answers)\n",
        "        else:\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2, questions, answers)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Prompt 1 Accuracy: {accuracy1:.2%}\")\n",
        "    print(f\"Prompt 2 Accuracy: {accuracy2:.2%}\")\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Question': questions,\n",
        "        'Correct Answer': answers,\n",
        "        'Prompt 1 Response': responses1,\n",
        "        'Prompt 1 Extracted': extracted1,\n",
        "        'Prompt 2 Response': responses2,\n",
        "        'Prompt 2 Extracted': extracted2,\n",
        "        'Prompt 1 Correct': [e == a for e, a in zip(extracted1, answers)],\n",
        "        'Prompt 2 Correct': [e == a for e, a in zip(extracted2, answers)]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv('prompt_comparison_results.csv', index=False)\n",
        "\n",
        "    print(\"\\nDetailed Statistics:\")\n",
        "    print(\"Prompt 1:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 1 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "    print(\"\\nPrompt 2:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 2 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QASC Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "\n",
        "PROMPT_1 = \"\"\"\n",
        "# Self reflection with ToT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. When answering questions, please generate a thought tree based on the content of the input question. This thought tree should contain reasoning paths, and the final answer to the question should be deduced based on multiple paths generated from the thought tree.\n",
        "2. Use as many **reasoning steps** as possible.   At least 3 steps.\n",
        "3. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "4. Include exploration of alternative answers.   Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "5. When you say you are rechecking, actually recheck and use another method.   Don't just say you are rechecking.\n",
        "6. Use at least 3 methods to arrive at the answer.\n",
        "7. Use best practices.\n",
        "8. Output the format of the answer: \"Answer\" Thinking tree: (This includes every step of the thinking tree and relation)\n",
        "\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step to verify its correctness. After completing each step, initiate a 'Review LLM' to examine the current step from different perspectives.\n",
        "1. For the answers and thought trees generated from the first step, please conduct a detailed review of each answer and thought tree, and apply a self-reflection mechanism to each step of the thought tree, using as many reasoning steps as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI, as well as what you can and cannot do.\n",
        "3. Include the exploration of different answers. Consider that you may be wrong, and identify where mistakes might occur if your reasoning is incorrect.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_2 = \"\"\"Hi there\"\"\"\n",
        "\n",
        "def create_answer_extraction_agent(llm):\n",
        "\n",
        "    def extract_final_number(text: str) -> str:\n",
        "        if \"####\" in text:\n",
        "            match = re.search(r\"####\\s*(-?\\d*\\.?\\d+)\", text)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        numbers = re.findall(r\"(-?\\d*\\.?\\d+)\", text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    extract_tool = FunctionTool.from_defaults(\n",
        "        fn=extract_final_number,\n",
        "        name=\"extract_final_number\",\n",
        "        description=\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\"\n",
        "    )\n",
        "\n",
        "    agent = ReActAgent.from_tools(\n",
        "        tools=[extract_tool],\n",
        "        llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return agent\n",
        "\n",
        "# 请下载 QASC 数据集，然后放到 dataset 目录下\n",
        "# 数据下载链接：https://github.com/allenai/qasc\n",
        "def load_qasc_dataset(path=\"./dataset/dev.jsonl\", num_samples=50):\n",
        "    questions = []\n",
        "    answers = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            questions.append(stem = data[\"question\"][\"stem\"])\n",
        "            choices = data[\"question\"][\"choices\"]\n",
        "            answer_key = data[\"answerKey\"]\n",
        "            correct_answer = None\n",
        "            for choice in choices:\n",
        "                if choice[\"label\"] == answer_key:\n",
        "                    correct_answer = choice[\"text\"]\n",
        "                    break\n",
        "            answer = correct_answer.split('####')[-1].strip()\n",
        "            answers.append(answer)\n",
        "            if len(questions) >= num_samples:\n",
        "                break\n",
        "    return questions[:5], answers[:5]\n",
        "\n",
        "def evaluate_prompt(system_prompt, questions, answers):\n",
        "    llm = Ollama(\n",
        "        model=\"llama3:8b\",  # llama3:8b, qwen2:7b, phi3:8b, gemma2:9b\n",
        "        temperature=0,\n",
        "        base_url=API_BASE,  # ollama default localhost: \"http://localhost:11434\" \n",
        "        system_prompt=system_prompt\n",
        "    )\n",
        "\n",
        "    agent = create_answer_extraction_agent(llm)\n",
        "\n",
        "    correct = 0\n",
        "    responses = []\n",
        "    extracted_answers = []\n",
        "\n",
        "    for q, a in tqdm(zip(questions, answers), total=len(questions)):\n",
        "        try:\n",
        "            response = llm.complete(q)\n",
        "            responses.append(response.text)\n",
        "\n",
        "            agent_response = agent.chat(\n",
        "                f\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\\n{response.text}\"\n",
        "            )\n",
        "            extracted_answer = agent_response.response.strip()\n",
        "            extracted_answers.append(extracted_answer)\n",
        "\n",
        "            if str(extracted_answer).strip() == str(a).strip():\n",
        "                correct += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {e}\")\n",
        "            responses.append(\"Error\")\n",
        "            extracted_answers.append(\"Error\")\n",
        "\n",
        "    accuracy = correct / len(questions)\n",
        "    return accuracy, responses, extracted_answers\n",
        "\n",
        "def main():\n",
        "    iteration = 5 # 3,5,7\n",
        "    questions, answers = load_qasc_dataset(num_samples=50)\n",
        "\n",
        "    print(\"Testing Prompt 1...\")\n",
        "    accuracy1, responses1, extracted1 = evaluate_prompt(PROMPT_1, questions, answers)\n",
        "\n",
        "    print(\"Testing Prompt 2...\")\n",
        "    accuracy2 = None\n",
        "    response2 = None\n",
        "    extracted2 = None\n",
        "    for i in range(iteration):\n",
        "        if(response2 is not None):\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2 + \"\\n\" + \"Generated response:\" + response2, questions, answers)\n",
        "        else:\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2, questions, answers)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Prompt 1 Accuracy: {accuracy1:.2%}\")\n",
        "    print(f\"Prompt 2 Accuracy: {accuracy2:.2%}\")\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Question': questions,\n",
        "        'Correct Answer': answers,\n",
        "        'Prompt 1 Response': responses1,\n",
        "        'Prompt 1 Extracted': extracted1,\n",
        "        'Prompt 2 Response': responses2,\n",
        "        'Prompt 2 Extracted': extracted2,\n",
        "        'Prompt 1 Correct': [e == a for e, a in zip(extracted1, answers)],\n",
        "        'Prompt 2 Correct': [e == a for e, a in zip(extracted2, answers)]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv('prompt_comparison_results.csv', index=False)\n",
        "\n",
        "    print(\"\\nDetailed Statistics:\")\n",
        "    print(\"Prompt 1:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 1 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "    print(\"\\nPrompt 2:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 2 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "2WikiMultiHopQA Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "\n",
        "PROMPT_1 = \"\"\"\n",
        "# Self reflection with ToT\n",
        "\n",
        "## Role\n",
        "\n",
        "You are an expert AI assistant capable of gradually explaining the reasoning process.\n",
        "\n",
        "## First Think step\n",
        "\n",
        "\n",
        "For each step, provide a title that describes what you did in that step, along with the corresponding content.\n",
        "Decide whether another step is needed or if you are ready to give the final answer.\n",
        "To improve instruction compliance, emphasize the importance of the instructions through `Markdown` syntax, including a set of tips and best practices:\n",
        "1. When answering questions, please generate a thought tree based on the content of the input question. This thought tree should contain reasoning paths, and the final answer to the question should be deduced based on multiple paths generated from the thought tree.\n",
        "2. Use as many **reasoning steps** as possible.   At least 3 steps.\n",
        "3. Be aware of your limitations as an AI and what you can and cannot do.\n",
        "4. Include exploration of alternative answers.   Consider that you might be wrong and where the error might be if your reasoning is incorrect.\n",
        "5. When you say you are rechecking, actually recheck and use another method.   Don't just say you are rechecking.\n",
        "6. Use at least 3 methods to arrive at the answer.\n",
        "7. Use best practices.\n",
        "8. Output the format of the answer: \"Answer\" Thinking tree: (This includes every step of the thinking tree and relation)\n",
        "\n",
        "\n",
        "## Second Think step\n",
        "\n",
        "\n",
        "For each step mentioned in the previous text, initiate a small sub-step to verify its correctness. After completing each step, initiate a 'Review LLM' to examine the current step from different perspectives.\n",
        "1. For the answers and thought trees generated from the first step, please conduct a detailed review of each answer and thought tree, and apply a self-reflection mechanism to each step of the thought tree, using as many reasoning steps as possible. At least three steps.\n",
        "2. Be aware of your limitations as an AI, as well as what you can and cannot do.\n",
        "3. Include the exploration of different answers. Consider that you may be wrong, and identify where mistakes might occur if your reasoning is incorrect.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_2 = \"\"\"Hi there\"\"\"\n",
        "\n",
        "def create_answer_extraction_agent(llm):\n",
        "\n",
        "    def extract_final_number(text: str) -> str:\n",
        "        if \"####\" in text:\n",
        "            match = re.search(r\"####\\s*(-?\\d*\\.?\\d+)\", text)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        numbers = re.findall(r\"(-?\\d*\\.?\\d+)\", text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    extract_tool = FunctionTool.from_defaults(\n",
        "        fn=extract_final_number,\n",
        "        name=\"extract_final_number\",\n",
        "        description=\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\"\n",
        "    )\n",
        "\n",
        "    agent = ReActAgent.from_tools(\n",
        "        tools=[extract_tool],\n",
        "        llm=llm,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return agent\n",
        "\n",
        "# 请下载 2WikiMultiHopQA 数据集，然后放到 dataset 目录下\n",
        "# 数据下载链接：https://github.com/Alab-NII/2wikimultihop\n",
        "def load_twowikimultihopqa_dataset(path=\"./dataset/dev.json\", num_samples=50):\n",
        "    questions = []\n",
        "    answers = []\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.loads(f)\n",
        "        for dataitem in data:\n",
        "            questions.append(dataitem.get['question'])\n",
        "            answer = dataitem.get['answer'].split('####')[-1].strip()\n",
        "            answers.append(answer)\n",
        "            if len(questions) >= num_samples:\n",
        "                break\n",
        "    return questions[:5], answers[:5]\n",
        "\n",
        "def evaluate_prompt(system_prompt, questions, answers):\n",
        "    llm = Ollama(\n",
        "        model=\"llama3:8b\",  # llama3:8b, qwen2:7b, phi3:8b, gemma2:9b\n",
        "        temperature=0,\n",
        "        base_url=API_BASE,  # ollama default localhost: \"http://localhost:11434\" \n",
        "        system_prompt=system_prompt\n",
        "    )\n",
        "\n",
        "    agent = create_answer_extraction_agent(llm)\n",
        "\n",
        "    correct = 0\n",
        "    responses = []\n",
        "    extracted_answers = []\n",
        "\n",
        "    for q, a in tqdm(zip(questions, answers), total=len(questions)):\n",
        "        try:\n",
        "            response = llm.complete(q)\n",
        "            responses.append(response.text)\n",
        "\n",
        "            agent_response = agent.chat(\n",
        "                f\"Extract the final numerical answer from the text. Only extract the numbers and do not output any additional explanatory text. Output only Arabic numerals.\\n{response.text}\"\n",
        "            )\n",
        "            extracted_answer = agent_response.response.strip()\n",
        "            extracted_answers.append(extracted_answer)\n",
        "\n",
        "            if str(extracted_answer).strip() == str(a).strip():\n",
        "                correct += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {e}\")\n",
        "            responses.append(\"Error\")\n",
        "            extracted_answers.append(\"Error\")\n",
        "\n",
        "    accuracy = correct / len(questions)\n",
        "    return accuracy, responses, extracted_answers\n",
        "\n",
        "def main():\n",
        "    iteration = 5 # 3,5,7\n",
        "    questions, answers = load_twowikimultihopqa_dataset(num_samples=50)\n",
        "\n",
        "    print(\"Testing Prompt 1...\")\n",
        "    accuracy1, responses1, extracted1 = evaluate_prompt(PROMPT_1, questions, answers)\n",
        "\n",
        "    print(\"Testing Prompt 2...\")\n",
        "    accuracy2 = None\n",
        "    response2 = None\n",
        "    extracted2 = None\n",
        "    for i in range(iteration):\n",
        "        if(response2 is not None):\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2 + \"\\n\" + \"Generated response:\" + response2, questions, answers)\n",
        "        else:\n",
        "            accuracy2, responses2, extracted2 = evaluate_prompt(PROMPT_2, questions, answers)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Prompt 1 Accuracy: {accuracy1:.2%}\")\n",
        "    print(f\"Prompt 2 Accuracy: {accuracy2:.2%}\")\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'Question': questions,\n",
        "        'Correct Answer': answers,\n",
        "        'Prompt 1 Response': responses1,\n",
        "        'Prompt 1 Extracted': extracted1,\n",
        "        'Prompt 2 Response': responses2,\n",
        "        'Prompt 2 Extracted': extracted2,\n",
        "        'Prompt 1 Correct': [e == a for e, a in zip(extracted1, answers)],\n",
        "        'Prompt 2 Correct': [e == a for e, a in zip(extracted2, answers)]\n",
        "    })\n",
        "\n",
        "    results_df.to_csv('prompt_comparison_results.csv', index=False)\n",
        "\n",
        "    print(\"\\nDetailed Statistics:\")\n",
        "    print(\"Prompt 1:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 1 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "    print(\"\\nPrompt 2:\")\n",
        "    print(f\"Total Correct: {sum(results_df['Prompt 2 Correct'])}\")\n",
        "    print(f\"Total Questions: {len(results_df)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
